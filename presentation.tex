\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
\usecolortheme{beaver}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}

\graphicspath{{figures/}}

\title{Predicting Insurance Costs: Analysis and Insights}
\author{SDS Mini-Datathon Team}
\institute[NUS]{National University of Singapore}
\date{\today}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Problem Framing \& Objective}
\begin{itemize}
\item \textbf{Challenge}: Predict medical insurance charges using demographic and lifestyle data
\item \textbf{Goal}: Build accurate regression models and identify key cost drivers
\item \textbf{Importance}: Fair pricing, risk assessment, healthcare insights
\item \textbf{Dataset}: 1,338 records with age, sex, BMI, children, smoker, region, charges
\item \textbf{Equity Context}: Recent studies highlight insurance pricing inequities
  \begin{itemize}
  \item NAIC Special Committee on Race and Insurance
  \item Concerns over credit scoring as proxy discriminator
  \item Our analysis ensures fairness through statistical testing
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Dataset and Features}
\small
\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Feature & Type & Notes \\
\midrule
age & integer & years \\
sex & categorical & male/female (encoded) \\
bmi & float & body mass index \\
children & integer & number of dependents \\
smoker & categorical & yes/no (encoded) \\
region & categorical & NE/NW/SE/SW (encoded) \\
\midrule
\textit{Engineered} & & \\
bmi\_category & categorical & underweight/normal/overweight/obese \\
age\_group & categorical & young/middle/senior \\
\bottomrule
\end{tabular}
\caption{Dataset schema and engineered features}
\end{table}
\end{frame}

\begin{frame}{Exploratory Data Analysis}
\begin{itemize}
\item Data quality: No missing values, clean dataset
\item Key statistics: Charges range \$1,122--\$63,771 (mean \$13,270)
\item Distribution: Right-skewed charges, normal BMI/age
\end{itemize}
\begin{figure}
\includegraphics[width=0.8\textwidth]{charges_dist.png}
\caption{Distribution of Insurance Charges}
\end{figure}
\end{frame}

\begin{frame}{Exploratory Data Analysis (Cont.)}
\begin{figure}
\includegraphics[width=0.8\textwidth]{correlation_heatmap.png}
\caption{Correlation Matrix}
\end{figure}
\begin{itemize}
\item Strong correlations: smoker (+0.79), age (+0.30), BMI (+0.20)
\item Regional variations in charges
\end{itemize}
\end{frame}

\begin{frame}{Preprocessing and Feature Engineering}
\begin{itemize}
  \item Label-encode: sex, smoker, region; derive bmi\_category and age\_group.
  \item Train/test split: 80/20 with fixed random seed for reproducibility.
  \item Standardize inputs with \texttt{StandardScaler} (fit on train, transform test).
  \item Keep target in original units (USD) to simplify interpretation of RMSE/MAE.
\end{itemize}
\end{frame}

\begin{frame}{Regression \& Modeling Approach}
\begin{itemize}
\item \textbf{Baseline Models}:
  \begin{itemize}
  \item Linear Regression: Interpretable, assumes linearity
  \item Decision Tree: Handles non-linearities, prone to overfitting
  \item Random Forest: Ensemble of trees (R²=0.865)
  \end{itemize}
\item \textbf{Advanced Optimization}:
  \begin{itemize}
  \item \textbf{Optuna}: Bayesian hyperparameter tuning (20 trials)
  \item \textbf{Random Forest (Optuna)}: Optimized forest (R²=0.879)
  \item \textbf{XGBoost (Optuna)}: Gradient boosting (R²=0.879)
  \end{itemize}
\item \textbf{Evaluation}: R², RMSE, MAE; 5-fold cross-validation
\end{itemize}
\end{frame}

\begin{frame}{Validation and Evaluation}
\begin{itemize}
  \item Validation: 5-fold cross-validation on the training data to assess variance.
  \item Metrics reported: $R^2$, RMSE, MAE (on the holdout test set).
\end{itemize}
\vspace{4pt}
\small
\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
Model & Mean $R^2$ (5-fold) & Std \\
\midrule
Linear Regression & 0.738 & 0.049 \\
Decision Tree & 0.720 & 0.067 \\
Random Forest & 0.825 & 0.043 \\
\bottomrule
\end{tabular}
\caption{Cross-validation performance (training folds)}
\end{table}
\end{frame}

\begin{frame}{Hyperparameter Tuning with Optuna}
\begin{itemize}
  \item Bayesian optimization (TPE) over 20 trials per model to maximize $R^2$ on validation.
  \item Search spaces summarized below; priors encourage compact trees and regularization to reduce overfitting.
\end{itemize}
\small
\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Model & Hyperparameter & Range \\
\midrule
Random Forest & n\_estimators & 200--800 \\
Random Forest & max\_depth & 4--16 \\
Random Forest & min\_samples\_split & 2--20 \\
Random Forest & min\_samples\_leaf & 1--20 \\
XGBoost & n\_estimators & 200--800 \\
XGBoost & max\_depth & 3--8 \\
XGBoost & learning\_rate & 0.01--0.3 \\
XGBoost & subsample & 0.6--1.0 \\
XGBoost & colsample\_bytree & 0.6--1.0 \\
XGBoost & gamma & 0.0--5.0 \\
XGBoost & reg\_alpha, reg\_lambda & 0.0--10.0 \\
\bottomrule
\end{tabular}
\caption{Optuna search spaces}
\end{table}
\end{frame}

\begin{frame}{Best Hyperparameters Found}
\small
\begin{columns}
\begin{column}{0.48\textwidth}
\textbf{Random Forest}
\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Param & Value \\
\midrule
n\_estimators & 567 \\
max\_depth & 5 \\
min\_samples\_split & 7 \\
min\_samples\_leaf & 8 \\
\bottomrule
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.52\textwidth}
\textbf{XGBoost}
\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Param & Value \\
\midrule
n\_estimators & 253 \\
max\_depth & 4 \\
learning\_rate & 0.0231 \\
subsample & 0.7301 \\
colsample\_bytree & 0.7555 \\
gamma & 1.3567 \\
reg\_alpha & 8.2874 \\
reg\_lambda & 3.5675 \\
\bottomrule
\end{tabular}
\end{table}
\end{column}
\end{columns}
\vspace{2pt}
Both tuned models achieve $R^2 \approx 0.879$ on the test set, improving over the untuned Random Forest.
\end{frame}

\begin{frame}{Key Findings \& Visualizations}
\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
Model & R² & RMSE (\$) & MAE (\$) \\
\midrule
Linear Regression & 0.787 & 5,747 & 4,097 \\
Decision Tree & 0.740 & 6,354 & 2,878 \\
Random Forest (Baseline) & 0.865 & 4,574 & 2,503 \\
\textbf{Random Forest (Optuna)} & \textbf{0.879} & \textbf{4,327} & \textbf{2,458} \\
\textbf{XGBoost (Optuna)} & \textbf{0.879} & \textbf{4,331} & \textbf{2,479} \\
\bottomrule
\end{tabular}
\caption{Model Performance Comparison}
\end{table}
\begin{itemize}
\item \textbf{Best Model}: Tuned tree ensembles cluster at $R^2 \approx 0.879$ with $<\$4.35$k RMSE
\item \textbf{Improvement}: +1.4 $R^2$ points over the untuned Random Forest
\item Cross-validation confirms stability
\end{itemize}
\end{frame}

\begin{frame}{Model Performance: Visual Comparison}
\begin{figure}
\includegraphics[width=0.9\textwidth]{model_comparison_all.png}
\caption{R$^2$ on test set across models (higher is better)}
\end{figure}
\end{frame}

\begin{frame}{Residual Diagnostics}
\begin{figure}
\includegraphics[width=0.92\textwidth]{residual_analysis.png}
\caption{Residuals vs predictions and residual distribution (tuned XGBoost)}
\end{figure}
\vspace{-6pt}
\small Mean residual $\approx -222$; standard deviation $\approx 4333$. Approximate symmetry and homoscedasticity are acceptable for pricing use cases.
\end{frame}

\begin{frame}{Permutation Importance (Random Forest)}
\begin{figure}
\includegraphics[width=0.9\textwidth]{feature_importance.png}
\caption{Permutation importance on the test split}
\end{figure}
\small Smoking, BMI, and Age dominate predictive power; this aligns with domain knowledge and SHAP analyses.
\end{frame}

\begin{frame}{Feature Impact Analysis}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{shap_summary.png}
\caption{SHAP Summary Plot}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{shap_bar.png}
\caption{SHAP Feature Importance}
\end{figure}
\end{column}
\end{columns}
\begin{itemize}
\item Top factors: Smoking (dominant), Age, BMI
\item Sex and region have minimal impact
\end{itemize}
\end{frame}

\begin{frame}{Partial Dependence Plots}
\begin{figure}
\includegraphics[width=0.95\textwidth]{pdp_core_features.png}
\caption{Marginal Effects of Key Features}
\end{figure}
\begin{itemize}
\item \textbf{Age}: Linear increase (\$8-10k from 20 to 60)
\item \textbf{BMI}: Gentle upward curve (accelerates above 30)
\item \textbf{Smoker}: Dramatic step function (+\$20k for smokers)
\end{itemize}
\end{frame}

\begin{frame}{Fairness Analysis}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{fairness_mean_pred_by_sex.png}
\caption{Charges by Sex}
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{fairness_mean_pred_by_region.png}
\caption{Charges by Region}
\end{figure}
\end{column}
\end{columns}
\begin{itemize}
\item \textbf{Sex}: Welch's t-test p=0.226 $\rightarrow$ \textcolor{green}{\checkmark} No significant bias
\item \textbf{Region}: ANOVA p=0.091 $\rightarrow$ \textcolor{green}{\checkmark} Minimal bias
\item Differences reflect genuine risk factors, not discrimination
\end{itemize}
\end{frame}

\begin{frame}{Practical Recommendations}
\begin{itemize}
\item \textbf{For Insurers}:
  \begin{itemize}
  \item Deploy XGBoost with Optuna for premium calculation
  \item \textbf{Highest ROI}: Smoking cessation programs (\$20k impact)
  \item Target obesity prevention (BMI effect accelerates >30)
  \end{itemize}
\item \textbf{Policy Implications}:
  \begin{itemize}
  \item Fair pricing based on verifiable risks validated via statistical tests
  \item Monitor regional healthcare access disparities
  \item Implement quarterly fairness audits (NAIC recommendations)
  \end{itemize}
\item \textbf{Future Work}:
  \begin{itemize}
  \item Collect longitudinal data, medical history
  \item Disparate impact analysis (AI Fairness 360, Fairlearn)
  \item Avoid controversial proxies (credit scores, ZIP codes)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Difficulties Faced \& Solutions}
\begin{itemize}
\item \textbf{Dataset Limitations}:
  \begin{itemize}
  \item Small size (1,338 records) limits generalizability
  \item Missing features: medical history, genetics, lifestyle details
  \item Static data: no temporal health changes
  \end{itemize}
\item \textbf{Technical Challenges}:
  \begin{itemize}
  \item XGBoost version conflicts: Resolved via conda-forge channel
  \item SHAP API changes: Updated to new waterfall/summary plot syntax
  \item Model overfitting: Addressed with Optuna's Bayesian optimization
  \item PDP compatibility: Ensured fitted estimator passed correctly
  \end{itemize}
\item \textbf{Solutions}:
  \begin{itemize}
  \item Feature engineering for better segmentation
  \item Rigorous validation and error analysis
  \item Innovative explainability techniques
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Conclusion}
\begin{itemize}
\item Successfully predicted \textbf{87.9\%} of insurance cost variance (tuned ensembles)
\item Smoking is the dominant factor (+\$20k), followed by age and BMI
\item Models validated as fair through statistical testing (p>0.05)
\item Full interpretability via SHAP and PDPs ensures transparency
\item Future: Larger datasets, disparate impact analysis, advanced AI
\end{itemize}
\begin{center}
\textbf{Thank you for your attention!}
\end{center}
\end{frame}

\end{document}
